# -*- coding: utf-8 -*-
"""fake_news.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mmnOJvmKUjo9WyfH_-all6zocIN4y_KS
"""

import numpy as np
import pandas as pd

# 데이터 불러오기

test = pd.read_csv("/content/drive/MyDrive/fake-news/test.csv")
train = pd.read_csv("/content/drive/MyDrive/fake-news/train.csv")
submit = pd.read_csv("/content/drive/MyDrive/fake-news/submit.csv")

test.head()

train.head()

submit.head()

train.text[0:1]

for i in range(5):
  print(f'{i+1}번째 기사')
  print(train.text[i])

print(train.shape)
print(test.shape)

train[train['label']==1]

train[train['label']==0]

train.isnull().sum()

test.isnull().sum()

# 비어 있는 곳은 다 빈칸으로 남겨두기.

train = train.fillna('')
test = test.fillna('')

"""### Stemming:
- Stemming은 linguistic normalisation 하는 과정이다. 
- 어근만 남기게 되어, 같은 어근을 가지고 있다면 동일한 언어로 취급할 수 있게 한다.


"""

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

# 불용어를 먼저 다운로드 받아 보자. 
nltk.download('stopwords')

print(stopwords.words('english'))
print(len(stopwords.words('english')), "개의 불용어가 있음.")

port_stem = PorterStemmer()

def stemming(content):
    review = re.sub('[^a-zA-Z]',' ',content)
    review = review.lower()
    review = review.split()
    review = [port_stem.stem(word) for word in review if not word in stopwords.words('english')]
    review = ' '.join(review)
    return review

train['content'] = train['author']+' '+train['title']

print(train['content'])

X = train.drop('label',axis=1)
X
Y = train['label']

Y

Y.value_counts()

train['content'] = train['content'].apply(stemming)
print(train['content'])

print(train['content'][0])

X = train['content'].values
Y = train['label'].values
X

"""### TF-IDF
- Term Frequency - Inverse Document Frequency
- 모든 문서에 자주 등장하는 단어는 중요도가 낮다.
- 특정 문서에서 자주 등장하는 단어는 중요도가 높다.

"""

#수치화가 필요함.

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer= TfidfVectorizer()
vectorizer.fit(X)
X = vectorizer.transform(X)

print(X)
print(Y)

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2, stratify=Y, random_state=2)

# stratify 는 classification을 다룰 때 매우 중요함. stratify를 target으로 지정해주면
# 각각의 class 비율을 train/validation에 유지해줍니다.

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train,Y_train)

from sklearn.metrics import accuracy_score

X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)

print(training_data_accuracy)

X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)
print(test_data_accuracy)

# Confusion matrix 그려보기

import matplotlib.pyplot as plt

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Purples):
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")
        
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

